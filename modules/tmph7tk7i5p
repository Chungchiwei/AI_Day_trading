"""
AI åˆ†ææ¨¡çµ„ - ä½¿ç”¨ Perplexity.ai é€²è¡Œç•¶æ²–åˆ†æ
å„ªåŒ–ç‰ˆï¼šåˆ†å±¤ä»»å‹™è™•ç†ã€æ¸›å°‘ Token æ¶ˆè€—
"""
import os
import requests
import json
import time
from typing import Optional, Dict, List, Literal
import pandas as pd
from modules.database import get_database
from dotenv import load_dotenv
load_dotenv()

PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")

class DayTradingAnalyzer:
    """ç•¶æ²– AI åˆ†æå™¨ - å„ªåŒ–ç‰ˆ"""
    
    # æ¨¡å‹é…ç½®ï¼ˆæŒ‰æˆæœ¬å’Œç”¨é€”åˆ†å±¤ï¼‰
    MODEL_CONFIG = {
        'news_search': {
            'model': 'llama-3.1-sonar-small-128k-online',  # æ–°èæœå°‹ç”¨å°æ¨¡å‹
            'max_tokens': 800,
            'temperature': 0.2,
            'description': 'æ–°èæœå°‹èˆ‡åˆç¯©'
        },
        'news_summary': {
            'model': 'llama-3.1-sonar-small-128k-online',  # æ–°èæ‘˜è¦ç”¨å°æ¨¡å‹
            'max_tokens': 500,
            'temperature': 0.1,
            'description': 'æ–°èæ‘˜è¦èˆ‡çµæ§‹åŒ–'
        },
        'technical_analysis': {
            'model': 'llama-3.1-sonar-small-128k-chat',  # ç´”æŠ€è¡“åˆ†æä¸éœ€è¦è¯ç¶²
            'max_tokens': 1500,
            'temperature': 0.3,
            'description': 'æŠ€è¡“é¢åˆ†æ'
        },
        'comprehensive_analysis': {
            'model': 'sonar-reasoning',  # ç¶œåˆåˆ†æç”¨æ¨ç†æ¨¡å‹
            'max_tokens': 2000,
            'temperature': 0.3,
            'description': 'ç¶œåˆåˆ†æèˆ‡æ±ºç­–'
        },
        'deep_research': {
            'model': 'sonar-deep-research',  # æ·±åº¦ç ”ç©¶ï¼ˆé™é‡ä½¿ç”¨ï¼‰
            'max_tokens': 3000,
            'temperature': 0.2,
            'description': 'æ·±åº¦ç ”ç©¶ï¼ˆé‡å¤§äº‹ä»¶ï¼‰'
        }
    }
    
    # æ–°èä¾†æºç™½åå–®ï¼ˆå°ç£ä¸»è¦è²¡ç¶“åª’é«”ï¼‰
    TRUSTED_DOMAINS = [
        'cnyes.com',           # é‰…äº¨ç¶²
        'money.udn.com',       # ç¶“æ¿Ÿæ—¥å ±
        'ctee.com.tw',         # å·¥å•†æ™‚å ±
        'wealth.com.tw',       # è²¡è¨Š
        'businessweekly.com.tw', # å•†æ¥­å‘¨åˆŠ
        'cw.com.tw',           # å¤©ä¸‹é›œèªŒ
        'mops.twse.com.tw',    # å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™
        'twse.com.tw'          # è­‰äº¤æ‰€
    ]
    
    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            api_key: Perplexity API Key
        """
        self.api_key = api_key or PERPLEXITY_API_KEY
        self.base_url = "https://api.perplexity.ai/chat/completions"
        self.db = get_database()
        
        if not self.api_key:
            raise ValueError("âŒ PERPLEXITY_API_KEY æœªè¨­å®š")
        
        # çµ±è¨ˆ Token ä½¿ç”¨é‡
        self.token_usage = {
            'prompt_tokens': 0,
            'completion_tokens': 0,
            'total_tokens': 0
        }
    
    def _call_api(self, messages: List[Dict], task_type: str = 'technical_analysis',
                  search_domain_filter: Optional[List[str]] = None,
                  max_retries: int = 2) -> Optional[Dict]:
        """
        å‘¼å« Perplexity APIï¼ˆå„ªåŒ–ç‰ˆï¼Œæ”¯æ´ä»»å‹™åˆ†å±¤ï¼‰
        
        Args:
            messages: å°è©±è¨Šæ¯åˆ—è¡¨
            task_type: ä»»å‹™é¡å‹ï¼ˆæ±ºå®šä½¿ç”¨å“ªå€‹æ¨¡å‹ï¼‰
            search_domain_filter: æœå°‹åŸŸåéæ¿¾å™¨
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
            
        Returns:
            åŒ…å«å›æ‡‰å’Œ token ä½¿ç”¨é‡çš„å­—å…¸
        """
        # ç²å–æ¨¡å‹é…ç½®
        config = self.MODEL_CONFIG.get(task_type, self.MODEL_CONFIG['technical_analysis'])
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": config['model'],
            "messages": messages,
            "temperature": config['temperature'],
            "max_tokens": config['max_tokens']
        }
        
        # æ·»åŠ æœå°‹åŸŸåéæ¿¾ï¼ˆåƒ…é™ online æ¨¡å‹ï¼‰
        if search_domain_filter and 'online' in config['model']:
            payload['search_domain_filter'] = search_domain_filter
        
        for attempt in range(max_retries):
            try:
                print(f"   ğŸ“¡ API è«‹æ±‚ [{config['description']}] (å˜—è©¦ {attempt + 1}/{max_retries})")
                print(f"   ğŸ¤– ä½¿ç”¨æ¨¡å‹: {config['model']}")
                
                response = requests.post(
                    self.base_url,
                    headers=headers,
                    json=payload,
                    timeout=120
                )
                
                response.raise_for_status()
                result = response.json()
                
                # è¨˜éŒ„ Token ä½¿ç”¨é‡
                if 'usage' in result:
                    usage = result['usage']
                    self.token_usage['prompt_tokens'] += usage.get('prompt_tokens', 0)
                    self.token_usage['completion_tokens'] += usage.get('completion_tokens', 0)
                    self.token_usage['total_tokens'] += usage.get('total_tokens', 0)
                    
                    print(f"   ğŸ’° Token ä½¿ç”¨: {usage.get('total_tokens', 0)} "
                          f"(Prompt: {usage.get('prompt_tokens', 0)}, "
                          f"Completion: {usage.get('completion_tokens', 0)})")
                
                return {
                    'content': result['choices'][0]['message']['content'],
                    'usage': result.get('usage', {}),
                    'model': config['model']
                }
                
            except requests.exceptions.Timeout:
                print(f"âš ï¸ è«‹æ±‚è¶…æ™‚ (å˜—è©¦ {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(5)
                    
            except requests.exceptions.RequestException as e:
                print(f"âŒ API è«‹æ±‚å¤±æ•—: {e}")
                if attempt < max_retries - 1:
                    time.sleep(3)
                    
            except Exception as e:
                print(f"âŒ æœªçŸ¥éŒ¯èª¤: {e}")
                break
        
        return None
    
    def search_news_events(self, symbol: str, force_update: bool = False,
                          use_deep_research: bool = False) -> Dict:
        """
        æœå°‹è‚¡ç¥¨è¿‘æœŸæ–°èäº‹ä»¶ï¼ˆå„ªåŒ–ç‰ˆï¼šåˆ†å…©éšæ®µè™•ç†ï¼‰
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            force_update: æ˜¯å¦å¼·åˆ¶æ›´æ–°
            use_deep_research: æ˜¯å¦ä½¿ç”¨æ·±åº¦ç ”ç©¶æ¨¡å‹ï¼ˆé‡å¤§äº‹ä»¶æ™‚ï¼‰
            
        Returns:
            dict: åŒ…å«æ–°èå…§å®¹å’Œå…ƒæ•¸æ“šçš„å­—å…¸
        """
        try:
            # å˜—è©¦å¾å¿«å–è®€å–
            if not force_update:
                cached_news = self.db.get_news_cache(symbol)
                if cached_news:
                    print(f"âœ… å¾å¿«å–è®€å– {symbol} çš„æ–°è")
                    return {
                        'content': cached_news,
                        'is_fallback': False,
                        'source': 'cache',
                        'model': 'cache'
                    }
            
            print(f"ğŸ” éšæ®µ 1: æœå°‹ {symbol} çš„æ–°èäº‹ä»¶...")
            
            # éšæ®µ 1: ä½¿ç”¨å°æ¨¡å‹æœå°‹å’Œåˆç¯©
            search_messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯å°è‚¡æ–°èæœå°‹åŠ©æ‰‹ã€‚è«‹æœå°‹ä¸¦åˆ—å‡ºæœ€é‡è¦çš„æ–°èæ¨™é¡Œå’Œä¾†æºï¼Œç”¨ç¹é«”ä¸­æ–‡ã€‚"
                },
                {
                    "role": "user",
                    "content": f"""æœå°‹å°è‚¡ {symbol} æœ€è¿‘ 7 å¤©çš„é‡è¦æ–°èï¼Œåªåˆ—å‡ºï¼š

1. æ¨™é¡Œ
2. æ—¥æœŸ
3. ä¾†æºç¶²ç«™

æ ¼å¼ï¼š
- [æ—¥æœŸ] æ¨™é¡Œ (ä¾†æº)

åªåˆ—å‡ºæœ€é‡è¦çš„ 3-5 å‰‡ï¼Œä¸è¦è©³ç´°å…§å®¹ã€‚"""
                }
            ]
            
            # ä½¿ç”¨å°æ¨¡å‹ + åŸŸåéæ¿¾
            search_result = self._call_api(
                search_messages,
                task_type='news_search',
                search_domain_filter=self.TRUSTED_DOMAINS
            )
            
            if not search_result:
                return self._get_fallback_news(symbol)
            
            raw_news = search_result['content']
            
            # æª¢æŸ¥æ˜¯å¦æœ‰é‡å¤§æ–°èï¼ˆé—œéµå­—åˆ¤æ–·ï¼‰
            important_keywords = ['è²¡å ±', 'ä½µè³¼', 'é‡è¨Š', 'æ³•èªª', 'å¢è³‡', 'æ¸›è³‡', 'åœç‰Œ']
            has_important_event = any(keyword in raw_news for keyword in important_keywords)
            
            # éšæ®µ 2: æ ¹æ“šé‡è¦æ€§é¸æ“‡æ¨¡å‹é€²è¡Œæ‘˜è¦
            print(f"ğŸ“ éšæ®µ 2: ç”Ÿæˆæ–°èæ‘˜è¦...")
            
            task_type = 'deep_research' if (use_deep_research and has_important_event) else 'news_summary'
            
            if has_important_event:
                print(f"   âš ï¸ åµæ¸¬åˆ°é‡å¤§äº‹ä»¶ï¼Œä½¿ç”¨{'æ·±åº¦ç ”ç©¶' if use_deep_research else 'æ¨™æº–'}æ¨¡å‹")
            
            summary_messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯å°è‚¡æ–°èåˆ†æå°ˆå®¶ã€‚è«‹å°‡æ–°èæ•´ç†æˆçµæ§‹åŒ–æ ¼å¼ï¼Œç”¨ç¹é«”ä¸­æ–‡ã€‚"
                },
                {
                    "role": "user",
                    "content": f"""è«‹å°‡ä»¥ä¸‹ {symbol} çš„æ–°èæ•´ç†æˆï¼š

{raw_news}

---
è¼¸å‡ºæ ¼å¼ï¼š

## ğŸ“° é‡è¦æ–°è ({symbol})

### [æ—¥æœŸ] æ¨™é¡Œ
- **é¡å‹**: è²¡å ±/ä½µè³¼/ç”¢æ¥­/å…¶ä»–
- **å½±éŸ¿**: æ­£é¢/è² é¢/ä¸­æ€§
- **é‡é»**: ä¸€å¥è©±èªªæ˜

---
åªä¿ç•™æœ€é‡è¦çš„ 2-3 å‰‡ï¼Œæ¯å‰‡ä¸è¶…é 50 å­—ã€‚"""
                }
            ]
            
            summary_result = self._call_api(
                summary_messages,
                task_type=task_type
            )
            
            if summary_result:
                news_content = summary_result['content']
                
                # å„²å­˜åˆ°å¿«å–
                self.db.save_news_cache(symbol, news_content)
                
                print(f"âœ… æ–°èåˆ†æå®Œæˆ (ä½¿ç”¨æ¨¡å‹: {summary_result['model']})")
                
                return {
                    'content': news_content,
                    'is_fallback': False,
                    'source': 'api',
                    'model': summary_result['model'],
                    'has_important_event': has_important_event
                }
            else:
                return self._get_fallback_news(symbol)
            
        except Exception as e:
            print(f"âŒ æœå°‹æ–°èäº‹ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return self._get_fallback_news(symbol)
    
    def _get_fallback_news(self, symbol: str) -> Dict:
        """å‚™ç”¨æ–°èè¨Šæ¯"""
        return {
            'content': f"""
## âš ï¸ æ–°èæœå°‹æš«æ™‚ç„¡æ³•ä½¿ç”¨

ç”±æ–¼ç¶²è·¯é€£ç·šå•é¡Œï¼Œç›®å‰ç„¡æ³•ç²å– {symbol} çš„æœ€æ–°æ–°èã€‚

### å»ºè­°æ›¿ä»£æ–¹æ¡ˆï¼š
1. æ‰‹å‹•æŸ¥è©¢ï¼š[é‰…äº¨ç¶²](https://news.cnyes.com/) æœå°‹ã€Œ{symbol}ã€
2. åƒè€ƒæŠ€è¡“é¢åˆ†æï¼ˆæœ¬ç³»çµ±ä»å¯ç”¨ï¼‰
3. ç¨å¾Œå‹¾é¸ã€Œå¼·åˆ¶æ›´æ–°ã€é‡è©¦

---
**æç¤º**: ç•¶æ²–äº¤æ˜“å»ºè­°ä»¥æŠ€è¡“é¢ç‚ºä¸»
""",
            'is_fallback': True,
            'source': 'fallback',
            'model': 'none'
        }
    
    def generate_daytrading_analysis(self, symbol: str, stock_data: pd.DataFrame, 
                                    today_open: float, yesterday_close: float,
                                    support_resistance: Dict, 
                                    institutional_data: Optional[pd.DataFrame] = None,
                                    news_events: Optional[Dict] = None,
                                    **kwargs) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„ç•¶æ²–æ±ºç­–åˆ†æï¼ˆå„ªåŒ–ç‰ˆï¼šåˆ†å±¤è™•ç†ï¼‰
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            stock_data: åŒ…å«æŠ€è¡“æŒ‡æ¨™çš„æ­·å²åƒ¹æ ¼æ•¸æ“š
            today_open: ç•¶æ—¥é–‹ç›¤åƒ¹
            yesterday_close: æ˜¨æ—¥æ”¶ç›¤åƒ¹
            support_resistance: æ”¯æ’å£“åŠ›ä½å­—å…¸
            institutional_data: ç±Œç¢¼æ•¸æ“š (å¯é¸)
            news_events: æ–°èäº‹ä»¶å­—å…¸ (å¯é¸)
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            AI ç”Ÿæˆçš„ç•¶æ²–åˆ†æå ±å‘Š
        """
        try:
            # é‡ç½® Token è¨ˆæ•¸
            self.token_usage = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
            
            # å¾ kwargs å–å¾—åƒæ•¸
            fee_discount = kwargs.get('fee_discount', 2.8)
            tax_rate = kwargs.get('tax_rate', 0.15)
            total_capital = kwargs.get('total_capital', 100000)
            risk_percent = kwargs.get('risk_percent', 1.0)
            
            # æº–å‚™åŸºæœ¬æ•¸æ“šï¼ˆç²¾ç°¡ç‰ˆï¼‰
            gap_amount = today_open - yesterday_close
            gap_percent = (gap_amount / yesterday_close) * 100
            
            latest = stock_data.iloc[-1]
            
            # åªä¿ç•™é—œéµæŠ€è¡“æŒ‡æ¨™ï¼ˆæ¸›å°‘ Tokenï¼‰
            key_indicators = {
                'close': float(latest.get('close', 0)),
                'MA5': float(latest.get('MA5', 0)),
                'MA20': float(latest.get('MA20', 0)),
                'RSI': float(latest.get('RSI', 0)),
                'MACD': float(latest.get('MACD', 0)),
                'KD_K': float(latest.get('KD_K', 0)),
                'volume_ratio': float(latest.get('volume', 0)) / stock_data['volume'].mean()
            }
            
            # ç²¾ç°¡æ”¯æ’å£“åŠ›ä½ï¼ˆåªå–æœ€è¿‘çš„ 2 å€‹ï¼‰
            resistance_list = support_resistance.get('resistance', [])[:2]
            support_list = support_resistance.get('support', [])[:2]
            
            resistance_text = " / ".join([f"NT$ {r['price']:.2f}" for r in resistance_list])
            support_text = " / ".join([f"NT$ {s['price']:.2f}" for s in support_list])
            
            # è™•ç†æ–°èï¼ˆç²¾ç°¡ç‰ˆï¼‰
            has_news = news_events is not None and not news_events.get('is_fallback', True)
            news_summary = ""
            
            if has_news:
                news_content = news_events.get('content', '')
                # åªå–æ–°èçš„å‰ 300 å­—
                news_summary = news_content[:300] + "..." if len(news_content) > 300 else news_content
            
            # ç²¾ç°¡ç±Œç¢¼æ•¸æ“š
            institutional_summary = ""
            if institutional_data is not None and not institutional_data.empty:
                latest_inst = institutional_data.iloc[-1]
                foreign = latest_inst.get('Foreign_Investor', 0)
                trust = latest_inst.get('Investment_Trust', 0)
                
                # åªå ±å‘Šé‡è¦çš„ç±Œç¢¼å‹•å‘
                if abs(foreign) > 1000 or abs(trust) > 500:
                    institutional_summary = f"å¤–è³‡ {foreign:+,.0f}å¼µ, æŠ•ä¿¡ {trust:+,.0f}å¼µ"
            
            # æ±ºå®šä½¿ç”¨å“ªå€‹æ¨¡å‹
            has_important_event = news_events.get('has_important_event', False) if has_news else False
            task_type = 'comprehensive_analysis' if has_news else 'technical_analysis'
            
            # æ§‹å»ºç²¾ç°¡æç¤ºè©
            system_message = "ä½ æ˜¯å°è‚¡ç•¶æ²–åˆ†æå°ˆå®¶ã€‚è«‹åŸºæ–¼æ•¸æ“šçµ¦å‡ºæ˜ç¢ºçš„äº¤æ˜“å»ºè­°ï¼Œç”¨ç¹é«”ä¸­æ–‡ï¼Œæ ¼å¼ç°¡æ½”ã€‚"
            
            user_prompt = f"""åˆ†æ {symbol} ç•¶æ²–æ©Ÿæœƒï¼š

**åƒ¹æ ¼**: æ˜¨æ”¶ {yesterday_close:.2f} â†’ ä»Šé–‹ {today_open:.2f} ({gap_percent:+.2f}%)

**æŠ€è¡“**: RSI {key_indicators['RSI']:.0f}, MACD {key_indicators['MACD']:.2f}, KD {key_indicators['KD_K']:.0f}
MA5 {key_indicators['MA5']:.2f} / MA20 {key_indicators['MA20']:.2f}

**é—œéµåƒ¹ä½**: 
å£“åŠ› {resistance_text} | æ”¯æ’ {support_text}
"""

            if institutional_summary:
                user_prompt += f"\n**ç±Œç¢¼**: {institutional_summary}"
            
            if has_news and news_summary:
                user_prompt += f"\n\n**æ–°è**: {news_summary}"
            
            user_prompt += f"""

**åƒæ•¸**: è³‡é‡‘ {total_capital:,}, é¢¨éšª {risk_percent}%, æ‰‹çºŒè²» {fee_discount}æŠ˜

---
è«‹æä¾›ï¼ˆç°¡æ½”ç‰ˆï¼‰ï¼š
1. é©åˆåº¦: âœ…é©åˆ / âš ï¸è¬¹æ… / âŒä¸å»ºè­°
2. é€²å ´åƒ¹: NT$ [åƒ¹æ ¼]
3. åœåˆ©åƒ¹: NT$ [åƒ¹æ ¼] (+X%)
4. åœæåƒ¹: NT$ [åƒ¹æ ¼] (-X%)
5. å»ºè­°å¼µæ•¸: [æ•¸å­—]å¼µ
6. é¢¨éšªæç¤º: [ä¸€å¥è©±]

è«‹ç›´æ¥çµ¦æ•¸å­—ï¼Œä¸è¦å†—é•·èªªæ˜ã€‚"""

            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_prompt}
            ]
            
            print(f"ğŸ¤– ç”Ÿæˆç•¶æ²–åˆ†æ ({'å«æ–°è' if has_news else 'ç´”æŠ€è¡“'})...")
            
            # èª¿ç”¨ API
            result = self._call_api(messages, task_type=task_type)
            
            if result:
                analysis = result['content']
                
                # æ·»åŠ æ¨¡å¼æ¨™ç±¤å’Œ Token ä½¿ç”¨çµ±è¨ˆ
                mode_tag = f"""## {'ğŸ“Š ç¶œåˆåˆ†æ' if has_news else 'âš¡ æŠ€è¡“åˆ†æ'}æ¨¡å¼

**ä½¿ç”¨æ¨¡å‹**: {result['model']}
**Token æ¶ˆè€—**: {self.token_usage['total_tokens']} 
(Prompt: {self.token_usage['prompt_tokens']}, Completion: {self.token_usage['completion_tokens']})

---

"""
                
                print(f"âœ… åˆ†æå®Œæˆ (ç¸½ Token: {self.token_usage['total_tokens']})")
                
                return mode_tag + analysis
            else:
                return self._get_fallback_analysis(symbol, today_open, yesterday_close, latest)
            
        except Exception as e:
            print(f"âŒ AI åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            import traceback
            print(traceback.format_exc())
            
            return f"""
## âŒ AI åˆ†æå¤±æ•—

éŒ¯èª¤è¨Šæ¯: {str(e)}

### å»ºè­°ï¼š
1. æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢º
2. ç¢ºèªç¶²è·¯é€£ç·š
3. åƒè€ƒæŠ€è¡“æŒ‡æ¨™æ‰‹å‹•åˆ†æ

---
æ‚¨ä»å¯ä½¿ç”¨ç³»çµ±çš„æŠ€è¡“åˆ†æåŠŸèƒ½ã€‚
"""
    
    def _get_fallback_analysis(self, symbol: str, today_open: float, 
                               yesterday_close: float, latest_data: pd.Series) -> str:
        """å‚™ç”¨åˆ†æ"""
        gap_percent = ((today_open - yesterday_close) / yesterday_close) * 100
        
        return f"""
## âš ï¸ AI åˆ†ææš«æ™‚ç„¡æ³•ä½¿ç”¨

### åŸºæœ¬æŠ€è¡“åˆ†æ ({symbol})

- æ˜¨æ”¶: NT$ {yesterday_close:.2f}
- ä»Šé–‹: NT$ {today_open:.2f} ({gap_percent:+.2f}%)
- RSI: {latest_data.get('RSI', 0):.2f}
- MA5: NT$ {latest_data.get('MA5', 0):.2f}
- KD_K: {latest_data.get('KD_K', 0):.2f}

### å»ºè­°
1. åƒè€ƒ K ç·šåœ–åˆ¤æ–·è¶¨å‹¢
2. æ³¨æ„æ”¯æ’å£“åŠ›ä½
3. åš´æ ¼åŸ·è¡Œåœæ

âš ï¸ ç•¶æ²–é¢¨éšªé«˜ï¼Œè«‹è¬¹æ…æ“ä½œ
"""
    
    def get_token_usage_summary(self) -> Dict:
        """ç²å– Token ä½¿ç”¨çµ±è¨ˆ"""
        return self.token_usage.copy()


# å‘å¾Œå…¼å®¹çš„å‡½æ•¸æ¥å£
def search_news_events(symbol: str, api_key: str, force_update: bool = False,
                      use_deep_research: bool = False) -> Dict:
    """å‘å¾Œå…¼å®¹çš„æ–°èæœå°‹å‡½æ•¸"""
    try:
        analyzer = DayTradingAnalyzer(api_key)
        return analyzer.search_news_events(symbol, force_update, use_deep_research)
    except Exception as e:
        print(f"âŒ æ–°èæœå°‹éŒ¯èª¤: {e}")
        return DayTradingAnalyzer(api_key)._get_fallback_news(symbol)


def generate_daytrading_analysis(symbol: str, stock_data: pd.DataFrame, 
                                 today_open: float, yesterday_close: float,
                                 support_resistance: Dict, 
                                 institutional_data: Optional[pd.DataFrame] = None,
                                 news_events: Optional[Dict] = None,
                                 api_key: Optional[str] = None,
                                 **kwargs) -> str:
    """å‘å¾Œå…¼å®¹çš„åˆ†æç”Ÿæˆå‡½æ•¸"""
    try:
        if not api_key:
            return """
## âš ï¸ AI åˆ†æåŠŸèƒ½æœªå•Ÿç”¨

è«‹è¨­å®š `PERPLEXITY_API_KEY` ç’°å¢ƒè®Šæ•¸ã€‚

### æ›¿ä»£æ–¹æ¡ˆï¼š
1. åƒè€ƒæŠ€è¡“æŒ‡æ¨™
2. åƒè€ƒæ”¯æ’å£“åŠ›ä½
3. æ‰‹å‹•æ±ºç­–

---
**æç¤º**: ç³»çµ±çš„æŠ€è¡“åˆ†æåŠŸèƒ½ä»å¯ä½¿ç”¨
"""
        
        analyzer = DayTradingAnalyzer(api_key)
        result = analyzer.generate_daytrading_analysis(
            symbol, stock_data, today_open, yesterday_close,
            support_resistance, institutional_data, news_events, **kwargs
        )
        
        # é¡¯ç¤º Token ä½¿ç”¨çµ±è¨ˆ
        usage = analyzer.get_token_usage_summary()
        print(f"\nğŸ’° æœ¬æ¬¡åˆ†æ Token ç¸½æ¶ˆè€—: {usage['total_tokens']}")
        
        return result
        
    except Exception as e:
        print(f"âŒ åˆ†æç”ŸæˆéŒ¯èª¤: {e}")
        return f"âŒ åˆ†æå¤±æ•—: {str(e)}"


if __name__ == "__main__":
    # æ¸¬è©¦
    api_key = os.getenv("PERPLEXITY_API_KEY")
    
    if api_key:
        try:
            analyzer = DayTradingAnalyzer(api_key)
            
            print("=" * 60)
            print("ğŸ§ª æ¸¬è©¦ AI åˆ†ææ¨¡çµ„ï¼ˆå„ªåŒ–ç‰ˆï¼‰")
            print("=" * 60)
            
            # æ¸¬è©¦æ–°èæœå°‹
            print("\n1ï¸âƒ£ æ¸¬è©¦æ–°èæœå°‹...")
            news = analyzer.search_news_events("2330", force_update=True)
            print(f"   ä¾†æº: {news['source']}")
            print(f"   æ¨¡å‹: {news.get('model', 'N/A')}")
            print(f"   å…§å®¹é è¦½: {news['content'][:150]}...")
            
            # é¡¯ç¤º Token ä½¿ç”¨
            usage = analyzer.get_token_usage_summary()
            print(f"\nğŸ’° Token ä½¿ç”¨çµ±è¨ˆ:")
            print(f"   Prompt: {usage['prompt_tokens']}")
            print(f"   Completion: {usage['completion_tokens']}")
            print(f"   Total: {usage['total_tokens']}")
            
            print("\nâœ… æ¸¬è©¦å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            print(traceback.format_exc())
    else:
        print("âŒ è«‹è¨­å®š PERPLEXITY_API_KEY ç’°å¢ƒè®Šæ•¸")
